{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gPQM1vkrLKs"
      },
      "source": [
        "# Prepare input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_F0EPGXuLL6",
        "outputId": "1be933a1-0af3-40a5-ac92-262442eea2b6"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda install -c rdkit rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "PNLNy5QirRTf",
        "outputId": "a1025465-31cd-4bbe-d9f3-454a8563fa2d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit import Chem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeSqw6RFrfS2"
      },
      "outputs": [],
      "source": [
        "# generate x\n",
        "\n",
        "trfile = open('/content/drive/MyDrive/FP2VEC_Tox21/tox21.csv', 'r')\n",
        "line = trfile.readline()\n",
        "dataX = []\n",
        "for i, line in enumerate(trfile):\n",
        "    line = line.rstrip().split(',')\n",
        "    smiles = str(line[13])\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
        "    fp = np.array(fp)\n",
        "    dataX.append(fp)\n",
        "trfile.close()\n",
        "\n",
        "dataX = np.array(dataX)\n",
        "np.save('tox21_fp', dataX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rpl8dlySrhwt"
      },
      "outputs": [],
      "source": [
        "# generate y\n",
        "\n",
        "dataY_concat = []\n",
        "for j in range(12):\n",
        "    trfile = open('/content/drive/MyDrive/FP2VEC_Tox21/tox21.csv', 'r')\n",
        "    line = trfile.readline()\n",
        "    dataY = []\n",
        "    for i, line in enumerate(trfile):\n",
        "        line = line.rstrip().split(',')\n",
        "        if not line[j]:\n",
        "            continue\n",
        "        val = float(line[j])\n",
        "        dataY.append(val)\n",
        "    dataY=np.array(dataY)\n",
        "    dataY_concat.append(dataY)\n",
        "    trfile.close()\n",
        "\n",
        "np.save('tox21_Y', dataY_concat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrZIm45tsjIq"
      },
      "source": [
        "#Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2bZTOQ2snzb"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit import Chem\n",
        "\n",
        "#tf.compat.v1.disable_v2_behavior()\n",
        "tf.compat.v1.disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiCLu6OesvWM"
      },
      "outputs": [],
      "source": [
        "# load data =========================================\n",
        "print('start loading data')\n",
        "dataX = np.load('tox21_fp.npy')\n",
        "dataY_concat  = np.load('tox21_Y.npy', allow_pickle=True)\n",
        "index = np.load('/content/drive/MyDrive/FP2VEC_Tox21/tox21_index.npy', allow_pickle=True) # 변경\n",
        "print(dataX.shape)\n",
        "\n",
        "dataX_concat = []\n",
        "for j in range(12):\n",
        "    dataX_concat.append([dataX[i] for i in index[j]])\n",
        "\n",
        "print('loading data is done!')\n",
        "# ===================================================\n",
        "start_time = time.time()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (8014, 1024) loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3UQ2VFIsyRK"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "Max_len = 200 # for padding\n",
        "embedding_size = 200\n",
        "n_hid = 1024 # number of feature maps\n",
        "win_size = 5 # window size of kernel\n",
        "lr = 1e-4 # learning rate of optimzier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynk2bc4GtARm"
      },
      "outputs": [],
      "source": [
        "# lookup table\n",
        "bit_size = 1024 # circular fingerprint\n",
        "emb = tf.Variable(tf.random.uniform([bit_size, embedding_size], -1, 1), dtype=tf.float32)\n",
        "pads = tf.constant([[1,0], [0,0]])\n",
        "embeddings = tf.pad(emb, pads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vguzzn5PtP6U"
      },
      "outputs": [],
      "source": [
        "# input shape pre-processing ========================\n",
        "data_x_concat = []\n",
        "data_y_concat = []\n",
        "\n",
        "for k in range(12):\n",
        "    data_x = []\n",
        "    data_y = []\n",
        "    for i in range(len(dataX_concat[k])):\n",
        "        fp = [0] * Max_len\n",
        "        n_ones = 0\n",
        "        for j in range(bit_size):\n",
        "            if dataX_concat[k][i][j] == 1:\n",
        "                fp[n_ones] = j+1\n",
        "                n_ones += 1\n",
        "        data_x.append(fp)\n",
        "        data_y.append([dataY_concat[k][i]])\n",
        "    data_x = np.array(data_x, dtype=np.int32)\n",
        "    data_y = np.array(data_y, dtype=np.float32)\n",
        "    data_x_concat.append(data_x)\n",
        "    data_y_concat.append(data_y)\n",
        "\n",
        "train_x_concat, test_x_concat, train_y_concat, test_y_concat, valid_x_concat, valid_y_concat = [], [], [], [], [], []\n",
        "for k in range(12):\n",
        "    train_x, test_x, train_y, test_y = train_test_split(data_x_concat[k], data_y_concat[k], test_size=0.1)\n",
        "    train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.1111)\n",
        "    train_x_concat.append(train_x)\n",
        "    test_x_concat.append(test_x)\n",
        "    train_y_concat.append(train_y)\n",
        "    test_y_concat.append(test_y)\n",
        "    valid_x_concat.append(valid_x)\n",
        "    valid_y_concat.append(valid_y)\n",
        "\n",
        "    print(train_x.shape, train_y.shape)\n",
        "    print(test_x.shape, test_y.shape)\n",
        "    print(valid_x.shape, valid_y.shape)\n",
        "\n",
        "train_size = [len(train_x_concat[i]) for i in range(len(train_x_concat))]\n",
        "test_size = [len(test_x_concat[i]) for i in range(len(test_x_concat))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x84JKIgqtSpo"
      },
      "outputs": [],
      "source": [
        "##################################################################\n",
        "# ================== CNN model construction ======================\n",
        "##################################################################\n",
        "\n",
        "def init_weights(shape):\n",
        "    return tf.Variable(tf.random.normal(shape, stddev = 0.01))\n",
        "def bias_variable(shape):\n",
        "    return tf.Variable(tf.constant(0.01, shape=shape))\n",
        "\n",
        "p_keep_conv = tf.compat.v1.placeholder(dtype=tf.float32)\n",
        "\n",
        "class model():\n",
        "    def __init__(self, embedding_size, n_hid, win_size, p_keep_conv, Max_len):\n",
        "         self.Max_len = Max_len\n",
        "         self.nhid  = n_hid\n",
        "         self.kernel_size = win_size\n",
        "         self.w2 = init_weights([self.kernel_size, embedding_size, 1, self.nhid]) # 64\n",
        "         self.w_o = init_weights([self.nhid, 1])\n",
        "\n",
        "         self.b2 = bias_variable([1, self.nhid])\n",
        "         self.b_o = bias_variable([1])\n",
        "         self.p_keep_conv = p_keep_conv\n",
        "\n",
        "    def conv_model(self, X):\n",
        "        l2 = tf.nn.relu(tf.nn.conv2d(X, self.w2, strides=[1, 1, 1, 1], padding='VALID') + self.b2)\n",
        "        l2 = tf.squeeze(l2, [2])\n",
        "        l2 = tf.nn.pool(l2, window_shape=[self.Max_len-self.kernel_size+1], pooling_type='MAX', padding='VALID')\n",
        "        l2 = tf.nn.dropout(l2, self.p_keep_conv)\n",
        "        lout = tf.reshape(l2, [-1, self.w_o.get_shape().as_list()[0]])\n",
        "        return lout\n",
        "\n",
        "X = tf.compat.v1.placeholder(tf.int32, [None, Max_len])\n",
        "Y = tf.compat.v1.placeholder(tf.float32, [None, 1])\n",
        "X_em = tf.nn.embedding_lookup(embeddings, X)\n",
        "X_em = tf.reshape(X_em, [-1, Max_len, embedding_size, 1])\n",
        "\n",
        "model = model(embedding_size, n_hid, win_size, p_keep_conv, Max_len)\n",
        "\n",
        "py_x = model.conv_model(X_em)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSHyArFytVpS"
      },
      "outputs": [],
      "source": [
        "# ==================== fully connected layers ===================== #\n",
        "temp_hid = n_hid\n",
        "w1 = init_weights([temp_hid, 1])\n",
        "w2 = init_weights([temp_hid, 1])\n",
        "w3 = init_weights([temp_hid, 1])\n",
        "w4 = init_weights([temp_hid, 1])\n",
        "w5 = init_weights([temp_hid, 1])\n",
        "w6 = init_weights([temp_hid, 1])\n",
        "w7 = init_weights([temp_hid, 1])\n",
        "w8 = init_weights([temp_hid, 1])\n",
        "w9 = init_weights([temp_hid, 1])\n",
        "w10 = init_weights([temp_hid, 1])\n",
        "w11 = init_weights([temp_hid, 1])\n",
        "w12 = init_weights([temp_hid, 1])\n",
        "\n",
        "b1 = bias_variable([1])\n",
        "b2 = bias_variable([1])\n",
        "b3 = bias_variable([1])\n",
        "b4 = bias_variable([1])\n",
        "b5 = bias_variable([1])\n",
        "b6 = bias_variable([1])\n",
        "b7 = bias_variable([1])\n",
        "b8 = bias_variable([1])\n",
        "b9 = bias_variable([1])\n",
        "b10 = bias_variable([1])\n",
        "b11 = bias_variable([1])\n",
        "b12 = bias_variable([1])\n",
        "\n",
        "py_x1 = tf.sigmoid(tf.matmul(py_x, w1) + b1)\n",
        "py_x2 = tf.sigmoid(tf.matmul(py_x, w2) + b2)\n",
        "py_x3 = tf.sigmoid(tf.matmul(py_x, w3) + b3)\n",
        "py_x4 = tf.sigmoid(tf.matmul(py_x, w4) + b4)\n",
        "py_x5 = tf.sigmoid(tf.matmul(py_x, w5) + b5)\n",
        "py_x6 = tf.sigmoid(tf.matmul(py_x, w6) + b6)\n",
        "py_x7 = tf.sigmoid(tf.matmul(py_x, w7) + b7)\n",
        "py_x8 = tf.sigmoid(tf.matmul(py_x, w8) + b8)\n",
        "py_x9 = tf.sigmoid(tf.matmul(py_x, w9) + b9)\n",
        "py_x10 = tf.sigmoid(tf.matmul(py_x, w10) + b10)\n",
        "py_x11 = tf.sigmoid(tf.matmul(py_x, w11) + b11)\n",
        "py_x12 = tf.sigmoid(tf.matmul(py_x, w12) + b12)\n",
        "\n",
        "cost1 = tf.compat.v1.losses.log_loss(labels=Y, predictions=py_x1)\n",
        "cost2 = tf.compat.v1.losses.log_loss(labels=Y, predictions=py_x2)\n",
        "cost3 = tf.compat.v1.losses.log_loss(labels=Y, predictions=py_x3)\n",
        "cost4 = tf.compat.v1.losses.log_loss(labels=Y, predictions=py_x4)\n",
        "cost5 = tf.compat.v1.losses.log_loss(labels=Y, predictions=py_x5)\n",
        "cost6 = tf.compat.v1.losses.log_loss(labels=Y, predictions=py_x6)\n",
        "cost7 = tf.compat.v1.losses.log_loss(labels=Y, predictions=py_x7)\n",
        "cost8 = tf.compat.v1.losses.log_loss(labels=Y, predictions=py_x8)\n",
        "cost9 = tf.compat.v1.losses.log_loss(labels=Y, predictions=py_x9)\n",
        "cost10 = tf.compat.v1.losses.log_loss(labels=Y, predictions=py_x10)\n",
        "cost11 = tf.compat.v1.losses.log_loss(labels=Y, predictions=py_x11)\n",
        "cost12 = tf.compat.v1.losses.log_loss(labels=Y, predictions=py_x12)\n",
        "\n",
        "train_op1 = tf.compat.v1.train.AdamOptimizer(learning_rate = lr).minimize(cost1)\n",
        "train_op2 = tf.compat.v1.train.AdamOptimizer(learning_rate = lr).minimize(cost2)\n",
        "train_op3 = tf.compat.v1.train.AdamOptimizer(learning_rate = lr).minimize(cost3)\n",
        "train_op4 = tf.compat.v1.train.AdamOptimizer(learning_rate = lr).minimize(cost4)\n",
        "train_op5 = tf.compat.v1.train.AdamOptimizer(learning_rate = lr).minimize(cost5)\n",
        "train_op6 = tf.compat.v1.train.AdamOptimizer(learning_rate = lr).minimize(cost6)\n",
        "train_op7 = tf.compat.v1.train.AdamOptimizer(learning_rate = lr).minimize(cost7)\n",
        "train_op8 = tf.compat.v1.train.AdamOptimizer(learning_rate = lr).minimize(cost8)\n",
        "train_op9 = tf.compat.v1.train.AdamOptimizer(learning_rate = lr).minimize(cost9)\n",
        "train_op10 = tf.compat.v1.train.AdamOptimizer(learning_rate = lr).minimize(cost10)\n",
        "train_op11 = tf.compat.v1.train.AdamOptimizer(learning_rate = lr).minimize(cost11)\n",
        "train_op12 = tf.compat.v1.train.AdamOptimizer(learning_rate = lr).minimize(cost12)\n",
        "\n",
        "prediction_error1 = cost1\n",
        "prediction_error2 = cost2\n",
        "prediction_error3 = cost3\n",
        "prediction_error4 = cost4\n",
        "prediction_error5 = cost5\n",
        "prediction_error6 = cost6\n",
        "prediction_error7 = cost7\n",
        "prediction_error8 = cost8\n",
        "prediction_error9 = cost9\n",
        "prediction_error10 = cost10\n",
        "prediction_error11 = cost11\n",
        "prediction_error12 = cost12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYaPB4mytb1q"
      },
      "outputs": [],
      "source": [
        "##################################################################\n",
        "# ==================== training part =============================\n",
        "##################################################################\n",
        "\n",
        "SAVER_DIR = \"model_tox21_multi\"\n",
        "saver = tf.compat.v1.train.Saver()\n",
        "ckpt_path = os.path.join(SAVER_DIR, \"model_tox21_multi\")\n",
        "ckpt = tf.train.get_checkpoint_state(SAVER_DIR)\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    best_auc = 0\n",
        "    best_idx = 0\n",
        "    for i in range(80):\n",
        "        training_batch = zip(range(0, len(train_x_concat[0]), batch_size),\n",
        "                             range(batch_size, len(train_x_concat[0])+1, batch_size))\n",
        "   #for start, end in tqdm.tqdm(training_batch):\n",
        "        for start, end in training_batch:\n",
        "            sess.run(train_op1, feed_dict={X: train_x_concat[0][start:end], Y: train_y_concat[0][start:end] ,p_keep_conv: 0.5})\n",
        "            sess.run(train_op2, feed_dict={X: train_x_concat[1][start:end], Y: train_y_concat[1][start:end] ,p_keep_conv: 0.5})\n",
        "            sess.run(train_op3, feed_dict={X: train_x_concat[2][start:end], Y: train_y_concat[2][start:end] ,p_keep_conv: 0.5})\n",
        "            sess.run(train_op4, feed_dict={X: train_x_concat[3][start:end], Y: train_y_concat[3][start:end] ,p_keep_conv: 0.5})\n",
        "            sess.run(train_op5, feed_dict={X: train_x_concat[4][start:end], Y: train_y_concat[4][start:end] ,p_keep_conv: 0.5})\n",
        "            sess.run(train_op6, feed_dict={X: train_x_concat[5][start:end], Y: train_y_concat[5][start:end] ,p_keep_conv: 0.5})\n",
        "            sess.run(train_op7, feed_dict={X: train_x_concat[6][start:end], Y: train_y_concat[6][start:end] ,p_keep_conv: 0.5})\n",
        "            sess.run(train_op8, feed_dict={X: train_x_concat[7][start:end], Y: train_y_concat[7][start:end] ,p_keep_conv: 0.5})\n",
        "            sess.run(train_op9, feed_dict={X: train_x_concat[8][start:end], Y: train_y_concat[8][start:end] ,p_keep_conv: 0.5})\n",
        "            sess.run(train_op10, feed_dict={X: train_x_concat[9][start:end], Y: train_y_concat[9][start:end] ,p_keep_conv: 0.5})\n",
        "            sess.run(train_op11, feed_dict={X: train_x_concat[10][start:end], Y: train_y_concat[10][start:end] ,p_keep_conv: 0.5})\n",
        "            sess.run(train_op12, feed_dict={X: train_x_concat[11][start:end], Y: train_y_concat[11][start:end] ,p_keep_conv: 0.5})\n",
        "\n",
        "   # print validation loss\n",
        "        merr = sess.run(prediction_error1, feed_dict={X: valid_x_concat[0], Y: valid_y_concat[0], p_keep_conv: 1.0})\n",
        "        print(i, merr, end = ' ')\n",
        "        merr = sess.run(prediction_error2, feed_dict={X: valid_x_concat[1], Y: valid_y_concat[1], p_keep_conv: 1.0})\n",
        "        print(merr, end = ' ')\n",
        "        merr = sess.run(prediction_error3, feed_dict={X: valid_x_concat[2], Y: valid_y_concat[2], p_keep_conv: 1.0})\n",
        "        print(merr, end = ' ')\n",
        "        merr = sess.run(prediction_error4, feed_dict={X: valid_x_concat[3], Y: valid_y_concat[3], p_keep_conv: 1.0})\n",
        "        print(merr, end = ' ')\n",
        "        merr = sess.run(prediction_error5, feed_dict={X: valid_x_concat[4], Y: valid_y_concat[4], p_keep_conv: 1.0})\n",
        "        print(merr, end = ' ')\n",
        "        merr = sess.run(prediction_error6, feed_dict={X: valid_x_concat[5], Y: valid_y_concat[5], p_keep_conv: 1.0})\n",
        "        print(merr, end = ' ')\n",
        "        merr = sess.run(prediction_error7, feed_dict={X: valid_x_concat[6], Y: valid_y_concat[6], p_keep_conv: 1.0})\n",
        "        print(merr, end = ' ')\n",
        "        merr = sess.run(prediction_error8, feed_dict={X: valid_x_concat[7], Y: valid_y_concat[7], p_keep_conv: 1.0})\n",
        "        print(merr, end = ' ')\n",
        "        merr = sess.run(prediction_error9, feed_dict={X: valid_x_concat[8], Y: valid_y_concat[8], p_keep_conv: 1.0})\n",
        "        print(merr, end = ' ')\n",
        "        merr = sess.run(prediction_error10, feed_dict={X: valid_x_concat[9], Y: valid_y_concat[9], p_keep_conv: 1.0})\n",
        "        print(merr, end = ' ')\n",
        "        merr = sess.run(prediction_error11, feed_dict={X: valid_x_concat[10], Y: valid_y_concat[10], p_keep_conv: 1.0})\n",
        "        print(merr, end = ' ')\n",
        "        merr = sess.run(prediction_error12, feed_dict={X: valid_x_concat[11], Y: valid_y_concat[11], p_keep_conv: 1.0})\n",
        "        print(merr)\n",
        "\n",
        "\n",
        "# calculate auc\n",
        "        val_preds1 = sess.run(py_x1, feed_dict={X: valid_x_concat[0], p_keep_conv: 1})\n",
        "        val_preds2 = sess.run(py_x2, feed_dict={X: valid_x_concat[1], p_keep_conv: 1})\n",
        "        val_preds3 = sess.run(py_x3, feed_dict={X: valid_x_concat[2], p_keep_conv: 1})\n",
        "        val_preds4 = sess.run(py_x4, feed_dict={X: valid_x_concat[3], p_keep_conv: 1})\n",
        "        val_preds5 = sess.run(py_x5, feed_dict={X: valid_x_concat[4], p_keep_conv: 1})\n",
        "        val_preds6 = sess.run(py_x6, feed_dict={X: valid_x_concat[5], p_keep_conv: 1})\n",
        "        val_preds7 = sess.run(py_x7, feed_dict={X: valid_x_concat[6], p_keep_conv: 1})\n",
        "        val_preds8 = sess.run(py_x8, feed_dict={X: valid_x_concat[7], p_keep_conv: 1})\n",
        "        val_preds9 = sess.run(py_x9, feed_dict={X: valid_x_concat[8], p_keep_conv: 1})\n",
        "        val_preds10 = sess.run(py_x10, feed_dict={X: valid_x_concat[9], p_keep_conv: 1})\n",
        "        val_preds11 = sess.run(py_x11, feed_dict={X: valid_x_concat[10], p_keep_conv: 1})\n",
        "        val_preds12 = sess.run(py_x12, feed_dict={X: valid_x_concat[11], p_keep_conv: 1})\n",
        "\n",
        "        val_aucs1 = roc_auc_score(valid_y_concat[0], val_preds1)\n",
        "        val_aucs2 = roc_auc_score(valid_y_concat[1], val_preds2)\n",
        "        val_aucs3 = roc_auc_score(valid_y_concat[2], val_preds3)\n",
        "        val_aucs4 = roc_auc_score(valid_y_concat[3], val_preds4)\n",
        "        val_aucs5 = roc_auc_score(valid_y_concat[4], val_preds5)\n",
        "        val_aucs6 = roc_auc_score(valid_y_concat[5], val_preds6)\n",
        "        val_aucs7 = roc_auc_score(valid_y_concat[6], val_preds7)\n",
        "        val_aucs8 = roc_auc_score(valid_y_concat[7], val_preds8)\n",
        "        val_aucs9 = roc_auc_score(valid_y_concat[8], val_preds9)\n",
        "        val_aucs10 = roc_auc_score(valid_y_concat[9], val_preds10)\n",
        "        val_aucs11 = roc_auc_score(valid_y_concat[10], val_preds11)\n",
        "        val_aucs12 = roc_auc_score(valid_y_concat[11], val_preds12)\n",
        "\n",
        "        val_aucs = [val_aucs1, val_aucs2, val_aucs3, val_aucs4, val_aucs5, val_aucs6, val_aucs7, val_aucs8, val_aucs9, val_aucs10, val_aucs11, val_aucs12]\n",
        "\n",
        "        print('mean validation auc: ', end = ' ')\n",
        "        print(np.mean(val_aucs))\n",
        "\n",
        "        if best_auc < np.mean(val_aucs):\n",
        "            auc1 = val_aucs1\n",
        "            auc2 = val_aucs2\n",
        "            auc3 = val_aucs3\n",
        "            auc4 = val_aucs4\n",
        "            auc5 = val_aucs5\n",
        "            auc6 = val_aucs6\n",
        "            auc7 = val_aucs7\n",
        "            auc8 = val_aucs8\n",
        "            auc9 = val_aucs9\n",
        "            auc10 = val_aucs10\n",
        "            auc11 = val_aucs11\n",
        "            auc12 = val_aucs12\n",
        "            best_auc = np.mean(val_aucs)\n",
        "            best_idx = i\n",
        "            save_path = saver.save(sess, ckpt_path, global_step = best_idx)\n",
        "            print('model saved!')\n",
        "            print()\n",
        "\n",
        "print('best epoch index: '+str(best_idx))\n",
        "print('best valid auc total: '+str(best_auc))\n",
        "print('best valid auc nr-ar: '+str(auc1))\n",
        "print('best valid auc nr-ar-lbd: '+str(auc2))\n",
        "print('best valid auc nr-ahr: '+str(auc3))\n",
        "print('best valid auc nr-aromatase: '+str(auc4))\n",
        "print('best valid auc nr-er: '+str(auc5))\n",
        "print('best valid auc nr-er-lbd: '+str(auc6))\n",
        "print('best valid auc nr-ppar-gamma: '+str(auc7))\n",
        "print('best valid auc sr-are: '+str(auc8))\n",
        "print('best valid auc sr-atad5: '+str(auc9))\n",
        "print('best valid auc sr-hse: '+str(auc10))\n",
        "print('best valid auc sr-mmp: '+str(auc11))\n",
        "print('best valid auc sr-p53: '+str(auc12))\n",
        "print(\"=== %s seconds ===\" % (time.time() - start_time))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "best valid auc total: 0.5009254840715515\n",
        "best valid auc nr-ar: 0.5\n",
        "best valid auc nr-ar-lbd: 0.5\n",
        "best valid auc nr-ahr: 0.5\n",
        "best valid auc nr-aromatase: 0.5\n",
        "best valid auc nr-er: 0.5\n",
        "best valid auc nr-er-lbd: 0.5\n",
        "best valid auc nr-ppar-gamma: 0.5\n",
        "best valid auc sr-are: 0.5\n",
        "best valid auc sr-atad5: 0.5\n",
        "best valid auc sr-hse: 0.5111058088586179\n",
        "best valid auc sr-mmp: 0.5\n",
        "best valid auc sr-p53: 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46h5BSm1tfZR"
      },
      "outputs": [],
      "source": [
        "####################################################################\n",
        "#=========================== test part ============================#\n",
        "####################################################################\n",
        "saver = tf.compat.v1.train.Saver()\n",
        "ckpt_path = os.path.join(SAVER_DIR, \"model_tox21_multi\")\n",
        "ckpt = tf.train.get_checkpoint_state(SAVER_DIR)\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    print(\"model loaded successfully!\")\n",
        "\n",
        "# test set\n",
        "    preds1 = sess.run(py_x1, feed_dict={X: test_x_concat[0], p_keep_conv: 1})\n",
        "    preds2 = sess.run(py_x2, feed_dict={X: test_x_concat[1], p_keep_conv: 1})\n",
        "    preds3 = sess.run(py_x3, feed_dict={X: test_x_concat[2], p_keep_conv: 1})\n",
        "    preds4 = sess.run(py_x4, feed_dict={X: test_x_concat[3], p_keep_conv: 1})\n",
        "    preds5 = sess.run(py_x5, feed_dict={X: test_x_concat[4], p_keep_conv: 1})\n",
        "    preds6 = sess.run(py_x6, feed_dict={X: test_x_concat[5], p_keep_conv: 1})\n",
        "    preds7 = sess.run(py_x7, feed_dict={X: test_x_concat[6], p_keep_conv: 1})\n",
        "    preds8 = sess.run(py_x8, feed_dict={X: test_x_concat[7], p_keep_conv: 1})\n",
        "    preds9 = sess.run(py_x9, feed_dict={X: test_x_concat[8], p_keep_conv: 1})\n",
        "    preds10 = sess.run(py_x10, feed_dict={X: test_x_concat[9], p_keep_conv: 1})\n",
        "    preds11 = sess.run(py_x11, feed_dict={X: test_x_concat[10], p_keep_conv: 1})\n",
        "    preds12 = sess.run(py_x12, feed_dict={X: test_x_concat[11], p_keep_conv: 1})\n",
        "\n",
        "    aucs1 = roc_auc_score(test_y_concat[0], preds1)\n",
        "    aucs2 = roc_auc_score(test_y_concat[1], preds2)\n",
        "    aucs3 = roc_auc_score(test_y_concat[2], preds3)\n",
        "    aucs4 = roc_auc_score(test_y_concat[3], preds4)\n",
        "    aucs5 = roc_auc_score(test_y_concat[4], preds5)\n",
        "    aucs6 = roc_auc_score(test_y_concat[5], preds6)\n",
        "    aucs7 = roc_auc_score(test_y_concat[6], preds7)\n",
        "    aucs8 = roc_auc_score(test_y_concat[7], preds8)\n",
        "    aucs9 = roc_auc_score(test_y_concat[8], preds9)\n",
        "    aucs10 = roc_auc_score(test_y_concat[9], preds10)\n",
        "    aucs11 = roc_auc_score(test_y_concat[10], preds11)\n",
        "    aucs12 = roc_auc_score(test_y_concat[11], preds12)\n",
        "\n",
        "    aucs = [aucs1, aucs2, aucs3, aucs4, aucs5, aucs6, aucs7, aucs8, aucs9, aucs10, aucs11, aucs12]\n",
        "    test_auc = np.mean(aucs)\n",
        "\n",
        "    print('test auc total: '+str(test_auc))\n",
        "    print('test auc nr-ar: '+str(aucs1))\n",
        "    print('test auc nr-ar-lbd: '+str(aucs2))\n",
        "    print('test auc nr-ahr: '+str(aucs3))\n",
        "    print('test auc nr-aromatase: '+str(aucs4))\n",
        "    print('test auc nr-er: '+str(aucs5))\n",
        "    print('test auc nr-er-lbd: '+str(aucs6))\n",
        "    print('test auc nr-ppar-gamma: '+str(aucs7))\n",
        "    print('test auc sr-are: '+str(aucs8))\n",
        "    print('test auc sr-atad5: '+str(aucs9))\n",
        "    print('test auc sr-hse: '+str(aucs10))\n",
        "    print('test auc sr-mmp: '+str(aucs11))\n",
        "    print('test auc sr-p53: '+str(aucs12))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
