{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geneformer Fine-Tuning for Classification of Dosage-Sensitive vs. -Insensitive Transcription Factors (TFs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GPU_NUMBER = [0]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(s) for s in GPU_NUMBER])\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import datetime\n",
    "import subprocess\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from geneformer import DataCollatorForGeneClassification\n",
    "from geneformer.pretrainer import token_dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Gene Attribute Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib numpy pandas datasets pytorch transformers tqdm scikit-learn ipykernel anndata scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genepy.utils import helper as h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching gene names from biomart cache\n"
     ]
    }
   ],
   "source": [
    "gene_info = h.getBiomartTable(\n",
    "    ensemble_server=\"http://jul2023.archive.ensembl.org/biomart\",\n",
    "    useCache=True,\n",
    "    attributes=[],\n",
    ")\n",
    "\n",
    "# table of corresponding Ensembl IDs, gene names, and gene types (e.g. coding, miRNA, etc.)\n",
    "\n",
    "# create dictionaries for corresponding attributes\n",
    "gene_id_type_dict = dict(zip(gene_info[\"ensembl_gene_id\"], gene_info[\"gene_biotype\"]))\n",
    "gene_name_id_dict = dict(zip(gene_info[\"hgnc_symbol\"], gene_info[\"ensembl_gene_id\"]))\n",
    "gene_id_name_dict = {v: k for k, v in gene_name_id_dict.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data and Class Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for preparing targets and labels\n",
    "def prep_inputs(genegroup1, genegroup2, id_type):\n",
    "    if id_type == \"gene_name\":\n",
    "        targets1 = [\n",
    "            gene_name_id_dict[gene]\n",
    "            for gene in genegroup1\n",
    "            if gene_name_id_dict.get(gene) in token_dictionary\n",
    "        ]\n",
    "        targets2 = [\n",
    "            gene_name_id_dict[gene]\n",
    "            for gene in genegroup2\n",
    "            if gene_name_id_dict.get(gene) in token_dictionary\n",
    "        ]\n",
    "    elif id_type == \"ensembl_id\":\n",
    "        targets1 = [gene for gene in genegroup1 if gene in token_dictionary]\n",
    "        targets2 = [gene for gene in genegroup2 if gene in token_dictionary]\n",
    "\n",
    "    targets1_id = [token_dictionary[gene] for gene in targets1]\n",
    "    targets2_id = [token_dictionary[gene] for gene in targets2]\n",
    "\n",
    "    targets = np.array(targets1_id + targets2_id)\n",
    "    labels = np.array([0] * len(targets1_id) + [1] * len(targets2_id))\n",
    "    nsplits = min(5, min(len(targets1_id), len(targets2_id)) - 1)\n",
    "    assert nsplits > 2\n",
    "    print(\n",
    "        f\"# targets1: {len(targets1_id)}\\n# targets2: {len(targets2_id)}\\n# splits: {nsplits}\"\n",
    "    )\n",
    "    return targets, labels, nsplits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# targets1: 122\n",
      "# targets2: 368\n",
      "# splits: 5\n"
     ]
    }
   ],
   "source": [
    "# preparing targets and labels for dosage sensitive vs insensitive TFs\n",
    "dosage_tfs = pd.read_csv(\"../dosage_sens_tf_labels.csv\", header=0)\n",
    "sensitive = dosage_tfs[\"dosage_sensitive\"].dropna()\n",
    "insensitive = dosage_tfs[\"dosage_insensitive\"].dropna()\n",
    "targets, labels, nsplits = prep_inputs(sensitive, insensitive, \"ensembl_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache-4c9ced29b01b4ba6.arrow  cache-a19f8cc37a0fd2a2.arrow  dataset.arrow\n",
      "cache-7671941302b3bea8.arrow  cache-b184d750f7a7f269.arrow  dataset_info.json\n",
      "cache-7c2a45dd1c07f3bb.arrow  cache-cfad106c0c760939.arrow  state.json\n"
     ]
    }
   ],
   "source": [
    "ls ../../Genecorpus-30M/genecorpus_30M_2048.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "# train_dataset = load_from_disk(\n",
    "#    \"../../Genecorpus-30M/example_input_files/gene_classification/bivalent_promoters/panglao_SRA553822-SRS2119548.dataset\"\n",
    "# )\n",
    "train_dataset = load_from_disk(\"../../Genecorpus-30M/genecorpus_30M_2048.dataset\")\n",
    "shuffled_train_dataset = train_dataset.shuffle(seed=42)\n",
    "subsampled_train_dataset = shuffled_train_dataset.select([i for i in range(30_000)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions for Training and Cross-Validating Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_classifier_batch(cell_batch, max_len):\n",
    "    if max_len == None:\n",
    "        max_len = max([len(i) for i in cell_batch[\"input_ids\"]])\n",
    "\n",
    "    def pad_label_example(example):\n",
    "        example[\"labels\"] = np.pad(\n",
    "            example[\"labels\"],\n",
    "            (0, max_len - len(example[\"input_ids\"])),\n",
    "            mode=\"constant\",\n",
    "            constant_values=-100,\n",
    "        )\n",
    "        example[\"input_ids\"] = np.pad(\n",
    "            example[\"input_ids\"],\n",
    "            (0, max_len - len(example[\"input_ids\"])),\n",
    "            mode=\"constant\",\n",
    "            constant_values=token_dictionary.get(\"<pad>\"),\n",
    "        )\n",
    "        example[\"attention_mask\"] = (\n",
    "            example[\"input_ids\"] != token_dictionary.get(\"<pad>\")\n",
    "        ).astype(int)\n",
    "        return example\n",
    "\n",
    "    padded_batch = cell_batch.map(pad_label_example)\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "# forward batch size is batch size for model inference (e.g. 200)\n",
    "def classifier_predict(model, evalset, forward_batch_size, mean_fpr):\n",
    "    predict_logits = []\n",
    "    predict_labels = []\n",
    "    model.eval()\n",
    "\n",
    "    # ensure there is at least 2 examples in each batch to avoid incorrect tensor dims\n",
    "    evalset_len = len(evalset)\n",
    "    max_divisible = find_largest_div(evalset_len, forward_batch_size)\n",
    "    if len(evalset) - max_divisible == 1:\n",
    "        evalset_len = max_divisible\n",
    "\n",
    "    max_evalset_len = max(evalset.select([i for i in range(evalset_len)])[\"length\"])\n",
    "\n",
    "    for i in range(0, evalset_len, forward_batch_size):\n",
    "        max_range = min(i + forward_batch_size, evalset_len)\n",
    "        batch_evalset = evalset.select([i for i in range(i, max_range)])\n",
    "        padded_batch = preprocess_classifier_batch(batch_evalset, max_evalset_len)\n",
    "        padded_batch.set_format(type=\"torch\")\n",
    "\n",
    "        input_data_batch = padded_batch[\"input_ids\"]\n",
    "        attn_msk_batch = padded_batch[\"attention_mask\"]\n",
    "        label_batch = padded_batch[\"labels\"]\n",
    "        model = model.to(\"cpu\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_data_batch.to(\"cpu\"),\n",
    "                attention_mask=attn_msk_batch.to(\"cpu\"),\n",
    "                labels=label_batch.to(\"cpu\"),\n",
    "            )\n",
    "            predict_logits += [torch.squeeze(outputs.logits.to(\"cpu\"))]\n",
    "            predict_labels += [torch.squeeze(label_batch.to(\"cpu\"))]\n",
    "\n",
    "    logits_by_cell = torch.cat(predict_logits)\n",
    "    all_logits = logits_by_cell.reshape(-1, logits_by_cell.shape[2])\n",
    "    labels_by_cell = torch.cat(predict_labels)\n",
    "    all_labels = torch.flatten(labels_by_cell)\n",
    "    logit_label_paired = [\n",
    "        item\n",
    "        for item in list(zip(all_logits.tolist(), all_labels.tolist()))\n",
    "        if item[1] != -100\n",
    "    ]\n",
    "    y_pred = [vote(item[0]) for item in logit_label_paired]\n",
    "    y_true = [item[1] for item in logit_label_paired]\n",
    "    logits_list = [item[0] for item in logit_label_paired]\n",
    "    # probability of class 1\n",
    "    y_score = [py_softmax(item)[1] for item in logits_list]\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    # plot roc_curve for this split\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC\")\n",
    "    plt.show()\n",
    "    # interpolate to graph\n",
    "    interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    return fpr, tpr, interp_tpr, conf_mat\n",
    "\n",
    "\n",
    "def vote(logit_pair):\n",
    "    a, b = logit_pair\n",
    "    if a > b:\n",
    "        return 0\n",
    "    elif b > a:\n",
    "        return 1\n",
    "    elif a == b:\n",
    "        return \"tie\"\n",
    "\n",
    "\n",
    "def py_softmax(vector):\n",
    "    e = np.exp(vector)\n",
    "    return e / e.sum()\n",
    "\n",
    "\n",
    "# get cross-validated mean and sd metrics\n",
    "def get_cross_valid_metrics(all_tpr, all_roc_auc, all_tpr_wt):\n",
    "    wts = [count / sum(all_tpr_wt) for count in all_tpr_wt]\n",
    "    print(wts)\n",
    "    all_weighted_tpr = [a * b for a, b in zip(all_tpr, wts)]\n",
    "    mean_tpr = np.sum(all_weighted_tpr, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    all_weighted_roc_auc = [a * b for a, b in zip(all_roc_auc, wts)]\n",
    "    roc_auc = np.sum(all_weighted_roc_auc)\n",
    "    roc_auc_sd = math.sqrt(np.average((all_roc_auc - roc_auc) ** 2, weights=wts))\n",
    "    return mean_tpr, roc_auc, roc_auc_sd\n",
    "\n",
    "\n",
    "# Function to find the largest number smaller\n",
    "# than or equal to N that is divisible by k\n",
    "def find_largest_div(N, K):\n",
    "    rem = N % K\n",
    "    if rem == 0:\n",
    "        return N\n",
    "    else:\n",
    "        return N - rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate gene classifier\n",
    "def cross_validate(\n",
    "    data,\n",
    "    targets,\n",
    "    labels,\n",
    "    nsplits,\n",
    "    subsample_size,\n",
    "    training_args,\n",
    "    freeze_layers,\n",
    "    output_dir,\n",
    "    num_proc,\n",
    "):\n",
    "    # check if output directory already written to\n",
    "    # ensure not overwriting previously saved model\n",
    "    model_dir_test = os.path.join(output_dir, \"ksplit0/models/pytorch_model.bin\")\n",
    "    if os.path.isfile(model_dir_test) == True:\n",
    "        raise Exception(\"Model already saved to this directory.\")\n",
    "\n",
    "    # initiate eval metrics to return\n",
    "    num_classes = len(set(labels))\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    all_roc_auc = []\n",
    "    all_tpr_wt = []\n",
    "    label_dicts = []\n",
    "    confusion = np.zeros((num_classes, num_classes))\n",
    "\n",
    "    # set up cross-validation splits\n",
    "    skf = StratifiedKFold(n_splits=nsplits, random_state=0, shuffle=True)\n",
    "    # train and evaluate\n",
    "    iteration_num = 0\n",
    "    for train_index, eval_index in tqdm(skf.split(targets, labels)):\n",
    "        if len(labels) > 500:\n",
    "            print(\"early stopping activated due to large # of training examples\")\n",
    "            nsplits = 3\n",
    "            if iteration_num == 3:\n",
    "                break\n",
    "        print(f\"****** Crossval split: {iteration_num}/{nsplits-1} ******\\n\")\n",
    "        # generate cross-validation splits\n",
    "        targets_train, targets_eval = targets[train_index], targets[eval_index]\n",
    "        labels_train, labels_eval = labels[train_index], labels[eval_index]\n",
    "        label_dict_train = dict(zip(targets_train, labels_train))\n",
    "        label_dict_eval = dict(zip(targets_eval, labels_eval))\n",
    "        label_dicts += (\n",
    "            iteration_num,\n",
    "            targets_train,\n",
    "            targets_eval,\n",
    "            labels_train,\n",
    "            labels_eval,\n",
    "        )\n",
    "\n",
    "        # function to filter by whether contains train or eval labels\n",
    "        def if_contains_train_label(example):\n",
    "            a = label_dict_train.keys()\n",
    "            b = example[\"input_ids\"]\n",
    "            return not set(a).isdisjoint(b)\n",
    "\n",
    "        def if_contains_eval_label(example):\n",
    "            a = label_dict_eval.keys()\n",
    "            b = example[\"input_ids\"]\n",
    "            return not set(a).isdisjoint(b)\n",
    "\n",
    "        # filter dataset for examples containing classes for this split\n",
    "        print(f\"Filtering training data\")\n",
    "        trainset = data.filter(if_contains_train_label, num_proc=num_proc)\n",
    "        print(\n",
    "            f\"Filtered {round((1-len(trainset)/len(data))*100)}%; {len(trainset)} remain\\n\"\n",
    "        )\n",
    "        print(f\"Filtering evalation data\")\n",
    "        evalset = data.filter(if_contains_eval_label, num_proc=num_proc)\n",
    "        print(\n",
    "            f\"Filtered {round((1-len(evalset)/len(data))*100)}%; {len(evalset)} remain\\n\"\n",
    "        )\n",
    "\n",
    "        # minimize to smaller training sample\n",
    "        training_size = min(subsample_size, len(trainset))\n",
    "        trainset_min = trainset.select([i for i in range(training_size)])\n",
    "        eval_size = min(training_size, len(evalset))\n",
    "        half_training_size = round(eval_size / 2)\n",
    "        evalset_train_min = evalset.select([i for i in range(half_training_size)])\n",
    "        evalset_oos_min = evalset.select(\n",
    "            [i for i in range(half_training_size, eval_size)]\n",
    "        )\n",
    "\n",
    "        # label conversion functions\n",
    "        def generate_train_labels(example):\n",
    "            example[\"labels\"] = [\n",
    "                label_dict_train.get(token_id, -100)\n",
    "                for token_id in example[\"input_ids\"]\n",
    "            ]\n",
    "            return example\n",
    "\n",
    "        def generate_eval_labels(example):\n",
    "            example[\"labels\"] = [\n",
    "                label_dict_eval.get(token_id, -100) for token_id in example[\"input_ids\"]\n",
    "            ]\n",
    "            return example\n",
    "\n",
    "        # label datasets\n",
    "        print(f\"Labeling training data\")\n",
    "        trainset_labeled = trainset_min.map(generate_train_labels)\n",
    "        print(f\"Labeling evaluation data\")\n",
    "        evalset_train_labeled = evalset_train_min.map(generate_eval_labels)\n",
    "        print(f\"Labeling evaluation OOS data\")\n",
    "        evalset_oos_labeled = evalset_oos_min.map(generate_eval_labels)\n",
    "\n",
    "        # create output directories\n",
    "        ksplit_output_dir = os.path.join(output_dir, f\"ksplit{iteration_num}\")\n",
    "        ksplit_model_dir = os.path.join(ksplit_output_dir, \"models/\")\n",
    "\n",
    "        # ensure not overwriting previously saved model\n",
    "        model_output_file = os.path.join(ksplit_model_dir, \"pytorch_model.bin\")\n",
    "        if os.path.isfile(model_output_file) == True:\n",
    "            raise Exception(\"Model already saved to this directory.\")\n",
    "\n",
    "        # make training and model output directories\n",
    "        subprocess.call(f\"mkdir {ksplit_output_dir}\", shell=True)\n",
    "        subprocess.call(f\"mkdir {ksplit_model_dir}\", shell=True)\n",
    "\n",
    "        # load model\n",
    "        model = BertForTokenClassification.from_pretrained(\n",
    "            \"../geneformer-12L-30M/\",\n",
    "            num_labels=2,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        if freeze_layers is not None:\n",
    "            modules_to_freeze = model.bert.encoder.layer[:freeze_layers]\n",
    "            for module in modules_to_freeze:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        model = model.to(\"cuda:0\")\n",
    "\n",
    "        # add output directory to training args and initiate\n",
    "        training_args[\"output_dir\"] = ksplit_output_dir\n",
    "        training_args_init = TrainingArguments(**training_args)\n",
    "\n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args_init,\n",
    "            data_collator=DataCollatorForGeneClassification(),\n",
    "            train_dataset=trainset_labeled,\n",
    "            eval_dataset=evalset_train_labeled,\n",
    "        )\n",
    "\n",
    "        # train the gene classifier\n",
    "        trainer.train()\n",
    "\n",
    "        # save model\n",
    "        trainer.save_model(ksplit_model_dir)\n",
    "\n",
    "        # evaluate model\n",
    "        fpr, tpr, interp_tpr, conf_mat = classifier_predict(\n",
    "            trainer.model, evalset_oos_labeled, 200, mean_fpr\n",
    "        )\n",
    "\n",
    "        # append to tpr and roc lists\n",
    "        confusion = confusion + conf_mat\n",
    "        all_tpr.append(interp_tpr)\n",
    "        all_roc_auc.append(auc(fpr, tpr))\n",
    "        # append number of eval examples by which to weight tpr in averaged graphs\n",
    "        all_tpr_wt.append(len(tpr))\n",
    "\n",
    "        iteration_num = iteration_num + 1\n",
    "\n",
    "    # get overall metrics for cross-validation\n",
    "    mean_tpr, roc_auc, roc_auc_sd = get_cross_valid_metrics(\n",
    "        all_tpr, all_roc_auc, all_tpr_wt\n",
    "    )\n",
    "    return all_roc_auc, roc_auc, roc_auc_sd, mean_fpr, mean_tpr, confusion, label_dicts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions for Plotting Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "def plot_ROC(bundled_data, title):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    for roc_auc, roc_auc_sd, mean_fpr, mean_tpr, sample, color in bundled_data:\n",
    "        plt.plot(\n",
    "            mean_fpr,\n",
    "            mean_tpr,\n",
    "            color=color,\n",
    "            lw=lw,\n",
    "            label=\"{0} (AUC {1:0.2f} $\\pm$ {2:0.2f})\".format(\n",
    "                sample, roc_auc, roc_auc_sd\n",
    "            ),\n",
    "        )\n",
    "    plt.plot([0, 1], [0, 1], color=\"black\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot confusion matrix\n",
    "def plot_confusion_matrix(classes_list, conf_mat, title):\n",
    "    display_labels = []\n",
    "    i = 0\n",
    "    for label in classes_list:\n",
    "        display_labels += [\"{0}\\nn={1:.0f}\".format(label, sum(conf_mat[:, i]))]\n",
    "        i = i + 1\n",
    "    display = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=preprocessing.normalize(conf_mat, norm=\"l1\"),\n",
    "        display_labels=display_labels,\n",
    "    )\n",
    "    display.plot(cmap=\"Blues\", values_format=\".2g\")\n",
    "    plt.title(title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune With Gene Classification Learning Objective and Quantify Predictive Performance\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please note that, as usual with deep learning models, we **highly** recommend tuning learning hyperparameters for all fine-tuning applications as this can significantly improve model performance. Example hyperparameters are defined below, but please see the \"hyperparam_optimiz_for_disease_classifier\" script for an example of how to tune hyperparameters for downstream applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model parameters\n",
    "# max input size\n",
    "max_input_size = 2**11  # 2048\n",
    "\n",
    "# set training hyperparameters\n",
    "# max learning rate\n",
    "max_lr = 5e-5\n",
    "# how many pretrained layers to freeze\n",
    "freeze_layers = 4\n",
    "# number gpus\n",
    "num_gpus = 1\n",
    "# number cpu cores\n",
    "num_proc = 12\n",
    "# batch size for training and eval\n",
    "geneformer_batch_size = 4\n",
    "# learning schedule\n",
    "lr_schedule_fn = \"linear\"\n",
    "# warmup steps\n",
    "warmup_steps = 500\n",
    "# number of epochs\n",
    "epochs = 1\n",
    "# optimizer\n",
    "optimizer = \"adamw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set training arguments\n",
    "subsample_size = 10_000\n",
    "training_args = {\n",
    "    \"learning_rate\": max_lr,\n",
    "    \"do_train\": True,\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"logging_steps\": 100,\n",
    "    \"group_by_length\": True,\n",
    "    \"length_column_name\": \"length\",\n",
    "    \"disable_tqdm\": False,\n",
    "    \"lr_scheduler_type\": lr_schedule_fn,\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    \"weight_decay\": 0.001,\n",
    "    #'device': \"cpu\",\n",
    "    \"per_device_train_batch_size\": geneformer_batch_size,\n",
    "    \"per_device_eval_batch_size\": geneformer_batch_size,\n",
    "    \"num_train_epochs\": epochs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r ../231024_geneformer_GeneClassifier_dosageTF*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define output directory path\n",
    "current_date = datetime.datetime.now()\n",
    "datestamp = (\n",
    "    f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
    ")\n",
    "training_output_dir = f\"../{datestamp}_geneformer_GeneClassifier_dosageTF_L{max_input_size}_B{geneformer_batch_size}_LR{max_lr}_LS{lr_schedule_fn}_WU{warmup_steps}_E{epochs}_O{optimizer}_n{subsample_size}_F{freeze_layers}/\"\n",
    "\n",
    "# ensure not overwriting previously saved model\n",
    "ksplit_model_test = os.path.join(\n",
    "    training_output_dir, \"ksplit0/models/pytorch_model.bin\"\n",
    ")\n",
    "if os.path.isfile(ksplit_model_test) == True:\n",
    "    raise Exception(\"Model already saved to this directory.\")\n",
    "\n",
    "# make output directory\n",
    "subprocess.call(f\"mkdir {training_output_dir}\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2138654e0eaa40879eda833829e8a0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Crossval split: 0/4 ******\n",
      "\n",
      "Filtering training data\n",
      "Filtered 36%; 19108 remain\n",
      "\n",
      "Filtering evalation data\n",
      "Filtered 50%; 15065 remain\n",
      "\n",
      "Labeling training data\n",
      "Labeling evaluation data\n",
      "Labeling evaluation OOS data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../geneformer-12L-30M/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjkobject\u001b[0m (\u001b[33mml4ig\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ml4ig1/Documents code/Geneformer/mytests/wandb/run-20231024_144509-nhnhd6va</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ml4ig/huggingface/runs/nhnhd6va' target=\"_blank\">kind-moon-16</a></strong> to <a href='https://wandb.ai/ml4ig/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ml4ig/huggingface' target=\"_blank\">https://wandb.ai/ml4ig/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ml4ig/huggingface/runs/nhnhd6va' target=\"_blank\">https://wandb.ai/ml4ig/huggingface/runs/nhnhd6va</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml4ig1/Documents code/Geneformer/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 19.70 GiB of which 6.00 MiB is free. Including non-PyTorch memory, this process has 4.00 GiB memory in use. Process 11383 has 15.24 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 13.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# cross-validate gene classifier\u001b[39;00m\n\u001b[1;32m      2\u001b[0m (\n\u001b[1;32m      3\u001b[0m     all_roc_auc,\n\u001b[1;32m      4\u001b[0m     roc_auc,\n\u001b[1;32m      5\u001b[0m     roc_auc_sd,\n\u001b[1;32m      6\u001b[0m     mean_fpr,\n\u001b[1;32m      7\u001b[0m     mean_tpr,\n\u001b[1;32m      8\u001b[0m     confusion,\n\u001b[1;32m      9\u001b[0m     label_dicts,\n\u001b[0;32m---> 10\u001b[0m ) \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m     11\u001b[0m     subsampled_train_dataset,\n\u001b[1;32m     12\u001b[0m     targets,\n\u001b[1;32m     13\u001b[0m     labels,\n\u001b[1;32m     14\u001b[0m     nsplits,\n\u001b[1;32m     15\u001b[0m     subsample_size,\n\u001b[1;32m     16\u001b[0m     training_args,\n\u001b[1;32m     17\u001b[0m     freeze_layers,\n\u001b[1;32m     18\u001b[0m     training_output_dir,\n\u001b[1;32m     19\u001b[0m     \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m )\n",
      "Cell \u001b[0;32mIn[12], line 149\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(data, targets, labels, nsplits, subsample_size, training_args, freeze_layers, output_dir, num_proc)\u001b[0m\n\u001b[1;32m    140\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m    141\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    142\u001b[0m     args\u001b[39m=\u001b[39mtraining_args_init,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m     eval_dataset\u001b[39m=\u001b[39mevalset_train_labeled,\n\u001b[1;32m    146\u001b[0m )\n\u001b[1;32m    148\u001b[0m \u001b[39m# train the gene classifier\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    151\u001b[0m \u001b[39m# save model\u001b[39;00m\n\u001b[1;32m    152\u001b[0m trainer\u001b[39m.\u001b[39msave_model(ksplit_model_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/trainer.py:2776\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2773\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2775\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2776\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2779\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/trainer.py:2801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2799\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2802\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1758\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m \u001b[39m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1758\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1759\u001b[0m     input_ids,\n\u001b[1;32m   1760\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1761\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1762\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1763\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1764\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1765\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1766\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1767\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1768\u001b[0m )\n\u001b[1;32m   1770\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1772\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    613\u001b[0m         hidden_states,\n\u001b[1;32m    614\u001b[0m         attention_mask,\n\u001b[1;32m    615\u001b[0m         layer_head_mask,\n\u001b[1;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    618\u001b[0m         past_key_value,\n\u001b[1;32m    619\u001b[0m         output_attentions,\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    498\u001b[0m         hidden_states,\n\u001b[1;32m    499\u001b[0m         attention_mask,\n\u001b[1;32m    500\u001b[0m         head_mask,\n\u001b[1;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[1;32m    430\u001b[0m         head_mask,\n\u001b[1;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    433\u001b[0m         past_key_value,\n\u001b[1;32m    434\u001b[0m         output_attentions,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:359\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    355\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    357\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(attention_probs)\n\u001b[1;32m    361\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/miniconda3/envs/geneformer/lib/python3.10/site-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[39m{\u001b[39;00mp\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 19.70 GiB of which 6.00 MiB is free. Including non-PyTorch memory, this process has 4.00 GiB memory in use. Process 11383 has 15.24 GiB memory in use. Of the allocated memory 3.77 GiB is allocated by PyTorch, and 13.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# cross-validate gene classifier\n",
    "(\n",
    "    all_roc_auc,\n",
    "    roc_auc,\n",
    "    roc_auc_sd,\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    confusion,\n",
    "    label_dicts,\n",
    ") = cross_validate(\n",
    "    subsampled_train_dataset,\n",
    "    targets,\n",
    "    labels,\n",
    "    nsplits,\n",
    "    subsample_size,\n",
    "    training_args,\n",
    "    freeze_layers,\n",
    "    training_output_dir,\n",
    "    1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bundle data for plotting\n",
    "bundled_data = []\n",
    "bundled_data += [(roc_auc, roc_auc_sd, mean_fpr, mean_tpr, \"Geneformer\", \"red\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "plot_ROC(bundled_data, \"Dosage Sensitive vs Insensitive TFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "classes_list = [\"Dosage Sensitive\", \"Dosage Insensitive\"]\n",
    "plot_confusion_matrix(classes_list, confusion, \"Geneformer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "eba1599a1f7e611c14c87ccff6793920aa63510b01fc0e229d6dd014149b8829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
