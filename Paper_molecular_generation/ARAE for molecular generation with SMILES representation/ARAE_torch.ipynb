{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toO3wASc_mKR"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import os,sys\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "def accu(pred,val,batch_l):\n",
        "\n",
        "    correct=0\n",
        "    total=0\n",
        "    cor_seq=0\n",
        "    for i in range(0,batch_l.shape[0]):\n",
        "        mm=(pred[i,0:batch_l[i]].cpu().data.numpy() == val[i,0:batch_l[i]].cpu().data.numpy())\n",
        "        correct+=mm.sum()\n",
        "        total+=batch_l[i].sum()\n",
        "        cor_seq+=mm.all()\n",
        "    acc=correct/float(total)\n",
        "    acc2=cor_seq/batch_l.shape[0]\n",
        "    return acc,acc2\n",
        "\n",
        "def vec_to_char(out_num):\n",
        "    stri=\"\"\n",
        "    for cha in out_num:\n",
        "        stri+=char_list[cha]\n",
        "    return stri\n",
        "\n",
        "def cal_prec_rec(Ypred,Ydata,conf):\n",
        "\n",
        "    small=0.0000000001\n",
        "    Ypred0=Ypred.cpu().data.numpy()\n",
        "    Ydata0=Ydata.cpu().data.numpy()\n",
        "    Ypred00=Ypred0>conf\n",
        "    mm=Ypred00*Ydata0\n",
        "    TP=mm.sum()\n",
        "    A=Ydata0.sum()\n",
        "    P=Ypred00.sum()\n",
        "    precision=(TP+small)/(P+small)\n",
        "    recall=(TP+small)/A\n",
        "\n",
        "    return precision, recall\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self,para,bias=True):\n",
        "        super(Encoder,self).__init__()\n",
        "\n",
        "        self.Nseq=para['Nseq']\n",
        "        self.Nfea=para['Nfea']\n",
        "\n",
        "        self.hidden_dim=para['hidden_dim']\n",
        "        self.NLSTM_layer=para['NLSTM_layer']\n",
        "\n",
        "        self.embedd = nn.Embedding(self.Nfea, self.Nfea)\n",
        "        self.encoder_rnn = nn.LSTM(input_size=self.Nfea,hidden_size=self.hidden_dim,\n",
        "                num_layers=self.NLSTM_layer,bias=True,\n",
        "                batch_first=True,bidirectional=False)\n",
        "\n",
        "        for param in self.encoder_rnn.parameters():\n",
        "            if len(param.shape)>=2:\n",
        "                nn.init.orthogonal_(param.data)\n",
        "            else:\n",
        "                nn.init.normal_(param.data)\n",
        "\n",
        "    def forward(self,X0,L0):\n",
        "\n",
        "        batch_size=X0.shape[0]\n",
        "        device=X0.device\n",
        "        enc_h0 = torch.zeros(self.NLSTM_layer*1,batch_size,self.hidden_dim).to(device)\n",
        "        enc_c0 = torch.zeros(self.NLSTM_layer*1,batch_size,self.hidden_dim).to(device)\n",
        "\n",
        "        X = self.embedd(X0)\n",
        "        out,(encoder_hn,encoder_cn)=self.encoder_rnn(X,(enc_h0,enc_c0))\n",
        "        last_step_index_list = (L0 - 1).view(-1, 1).expand(out.size(0), out.size(2)).unsqueeze(1)\n",
        "        Z=out.gather(1,last_step_index_list).squeeze()\n",
        "#        Z=torch.sigmoid(Z)\n",
        "        Z=F.normalize(Z,p=2,dim=1)\n",
        "\n",
        "        return Z\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self,para,bias=True):\n",
        "        super(Decoder,self).__init__()\n",
        "\n",
        "        self.Nseq=para['Nseq']\n",
        "        self.Nfea=para['Nfea']\n",
        "\n",
        "        self.hidden_dim=para['hidden_dim']\n",
        "        self.NLSTM_layer=para['NLSTM_layer']\n",
        "\n",
        "        self.embedd = nn.Embedding(self.Nfea, self.Nfea)\n",
        "\n",
        "#        self.decoder_rnn = nn.LSTM(input_size=self.Nfea,\n",
        "        self.decoder_rnn = nn.LSTM(input_size=self.Nfea+self.hidden_dim,\n",
        "            hidden_size=self.hidden_dim, num_layers=self.NLSTM_layer,\n",
        "            bias=True, batch_first=True,bidirectional=False)\n",
        "\n",
        "        for param in self.decoder_rnn.parameters():\n",
        "            if len(param.shape)>=2:\n",
        "                nn.init.orthogonal_(param.data)\n",
        "            else:\n",
        "                nn.init.normal_(param.data)\n",
        "\n",
        "        self.decoder_fc1=nn.Linear(self.hidden_dim,self.Nfea)\n",
        "        nn.init.xavier_normal_(self.decoder_fc1.weight.data)\n",
        "        nn.init.normal_(self.decoder_fc1.bias.data)\n",
        "\n",
        "    def forward(self, Z, X0, L0):\n",
        "\n",
        "        batch_size=Z.shape[0]\n",
        "        device=Z.device\n",
        "        dec_h0 = torch.zeros(self.NLSTM_layer*1,batch_size,self.hidden_dim).to(device)\n",
        "        dec_c0 = torch.zeros(self.NLSTM_layer*1,batch_size,self.hidden_dim).to(device)\n",
        "\n",
        "        X = self.embedd(X0)\n",
        "        Zm=Z.view(-1,1,self.hidden_dim).expand(-1,self.Nseq,self.hidden_dim)\n",
        "        ZX=torch.cat((Zm,X),2)\n",
        "\n",
        "#        dec_out,(decoder_hn,decoder_cn)=self.decoder_rnn(X0,(Z.view(1,-1,self.hidden_dim),dec_c0))\n",
        "        dec_out,(decoder_hn,decoder_cn)=self.decoder_rnn(ZX,(dec_h0,dec_c0))\n",
        "        dec=self.decoder_fc1(dec_out)\n",
        "        return dec\n",
        "\n",
        "    def decoding(self, Z):\n",
        "        batch_size=Z.shape[0]\n",
        "        device=Z.device\n",
        "        dec_h0 = torch.zeros(self.NLSTM_layer*1,batch_size,self.hidden_dim).to(device)\n",
        "        dec_c0 = torch.zeros(self.NLSTM_layer*1,batch_size,self.hidden_dim).to(device)\n",
        "\n",
        "        seq=torch.zeros([batch_size,1],dtype=torch.long).to(device)\n",
        "        seq[:,0]=self.Nfea-2\n",
        "\n",
        "#        Xdata_onehot=torch.zeros([batch_size,1,self.Nfea],dtype=torch.float32).to(device)\n",
        "#        Xdata_onehot[:,0,self.Nfea-2]=1\n",
        "        Y = seq\n",
        "        Zm=Z.view(-1,1,self.hidden_dim).expand(-1,1,self.hidden_dim)\n",
        "\n",
        "        decoder_hn=dec_h0\n",
        "        decoder_cn=dec_c0\n",
        "#        seq2=Xdata_onehot\n",
        "        for i in range(self.Nseq):\n",
        "            dec_h0=decoder_hn\n",
        "            dec_c0=decoder_cn\n",
        "\n",
        "            X = self.embedd(Y)\n",
        "            ZX=torch.cat((Zm,X),2)\n",
        "            dec_out,(decoder_hn,decoder_cn)=self.decoder_rnn(ZX,(dec_h0,dec_c0))\n",
        "            dec=self.decoder_fc1(dec_out)\n",
        "            Y= torch.argmax(dec,dim=2)\n",
        "#            Xdata_onehot=torch.zeros([batch_size,self.Nfea],dtype=torch.float32).to(device)\n",
        "#            Xdata_onehot=Xdata_onehot.scatter_(1,Y,1).view(-1,1,self.Nfea)\n",
        "            seq=torch.cat((seq,Y),dim=1)\n",
        "#            seq2=torch.cat((seq2,dec),dim=1)\n",
        "\n",
        "        return seq #, seq2[:,1:]\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self,para,bias=True):\n",
        "        super(Generator,self).__init__()\n",
        "\n",
        "        self.seed_dim=para['seed_dim']\n",
        "        self.hidden_dim=para['hidden_dim']\n",
        "\n",
        "        self.generator_fc1=nn.Linear(self.seed_dim,self.hidden_dim)\n",
        "        nn.init.xavier_normal_(self.generator_fc1.weight.data)\n",
        "        nn.init.normal_(self.generator_fc1.bias.data)\n",
        "\n",
        "        self.generator_fc2=nn.Linear(self.hidden_dim,self.hidden_dim)\n",
        "        nn.init.xavier_normal_(self.generator_fc2.weight.data)\n",
        "        nn.init.normal_(self.generator_fc2.bias.data)\n",
        "\n",
        "        self.generator_fc3=nn.Linear(self.hidden_dim,self.hidden_dim)\n",
        "        nn.init.xavier_normal_(self.generator_fc3.weight.data)\n",
        "        nn.init.normal_(self.generator_fc3.bias.data)\n",
        "\n",
        "    def forward(self,S0):\n",
        "\n",
        "        S1=self.generator_fc1(S0)\n",
        "        S1=torch.relu(S1)\n",
        "        S2=self.generator_fc2(S1)\n",
        "        S2=torch.relu(S2)\n",
        "        Zgen=self.generator_fc3(S2)\n",
        "#        Zgen=torch.sigmoid(Zgen)\n",
        "        Zgen=F.normalize(Zgen,p=2,dim=1)\n",
        "\n",
        "        return Zgen\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self,para,bias=True):\n",
        "        super(Critic,self).__init__()\n",
        "\n",
        "        self.hidden_dim=para['hidden_dim']\n",
        "\n",
        "        self.critic_fc1=nn.Linear(self.hidden_dim,self.hidden_dim)\n",
        "        nn.init.xavier_normal_(self.critic_fc1.weight.data)\n",
        "        nn.init.normal_(self.critic_fc1.bias.data)\n",
        "\n",
        "        self.critic_fc2=nn.Linear(self.hidden_dim,self.hidden_dim)\n",
        "        nn.init.xavier_normal_(self.critic_fc2.weight.data)\n",
        "        nn.init.normal_(self.critic_fc2.bias.data)\n",
        "\n",
        "        self.critic_fc3=nn.Linear(self.hidden_dim,1)\n",
        "        nn.init.xavier_normal_(self.critic_fc3.weight.data)\n",
        "        nn.init.normal_(self.critic_fc3.bias.data)\n",
        "\n",
        "    def forward(self,Z0):\n",
        "\n",
        "        D1=self.critic_fc1(Z0)\n",
        "        D1=torch.relu(D1)\n",
        "        D2=self.critic_fc2(D1)\n",
        "        D2=torch.relu(D2)\n",
        "        Dout=self.critic_fc3(D2)\n",
        "\n",
        "        return Dout\n",
        "\n",
        "    def clip(self,epsi=0.01):\n",
        "        torch.clamp_(self.critic_fc1.weight.data,min=-epsi,max=epsi)\n",
        "        torch.clamp_(self.critic_fc1.bias.data,min=-epsi,max=epsi)\n",
        "        torch.clamp_(self.critic_fc2.weight.data,min=-epsi,max=epsi)\n",
        "        torch.clamp_(self.critic_fc2.bias.data,min=-epsi,max=epsi)\n",
        "        torch.clamp_(self.critic_fc3.weight.data,min=-epsi,max=epsi)\n",
        "        torch.clamp_(self.critic_fc3.bias.data,min=-epsi,max=epsi)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self,para,bias=True):\n",
        "        super(Net,self).__init__()\n",
        "\n",
        "        self.Nseq=para['Nseq']\n",
        "        self.Nfea=para['Nfea']\n",
        "\n",
        "        self.hidden_dim=para['hidden_dim']\n",
        "        self.NLSTM_layer=para['NLSTM_layer']\n",
        "\n",
        "        self.Enc=Encoder(para)\n",
        "        self.Dec=Decoder(para)\n",
        "        self.Gen=Generator(para)\n",
        "        self.Cri=Critic(para)\n",
        "\n",
        "\n",
        "    def AE(self, X0, L0, noise):\n",
        "\n",
        "        Z = self.Enc(X0, L0)\n",
        "#        print(Z.shape, noise.shape)\n",
        "        Zn = Z+noise\n",
        "        decoded = self.Dec(Zn, X0, L0)\n",
        "\n",
        "        return decoded\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    print(\"main\")\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "valid.py"
      ],
      "metadata": {
        "id": "j8z73NfNAQYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem as AllChem\n",
        "from rdkit.Chem.QED import qed\n",
        "from rdkit.Chem.Descriptors import MolWt, MolLogP, NumHDonors, NumHAcceptors, TPSA\n",
        "from rdkit.Chem.rdMolDescriptors import CalcNumRotatableBonds\n",
        "from rdkit.Chem import MolStandardize\n",
        "#from molvs import tautomer\n",
        "from rdkit import DataStructs\n",
        "\n",
        "from multiprocessing import Manager\n",
        "from multiprocessing import Process\n",
        "from multiprocessing import Queue\n",
        "\n",
        "import sascorer\n",
        "\n",
        "USAGE = \"\"\"\n",
        "valid.py data_dir\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def creator(q, data, Nproc):\n",
        "    Ndata = len(data)\n",
        "    for d in data:\n",
        "        idx = d[0]\n",
        "        smiles = d[1]\n",
        "        q.put((idx, smiles))\n",
        "\n",
        "    for i in range(0, Nproc):\n",
        "        q.put('DONE')\n",
        "\n",
        "\n",
        "def check_validity(q, return_dict_valid):\n",
        "\n",
        "    while True:\n",
        "        qqq = q.get()\n",
        "        if qqq == 'DONE':\n",
        "            #            print('proc =', os.getpid())\n",
        "            break\n",
        "        idx, smi0 = qqq\n",
        "\n",
        "        index = smi0.find('>')\n",
        "        smi = smi0[0:index].strip('<')\n",
        "\n",
        "        if idx % 10000 == 0:\n",
        "            print(idx)\n",
        "\n",
        "        m = Chem.MolFromSmiles(smi)\n",
        "        if m is None:\n",
        "            continue\n",
        "        if Chem.SanitizeMol(m, catchErrors=True):\n",
        "            continue\n",
        "        smi2 = Chem.MolToSmiles(m)\n",
        "#        smi2=MolStandardize.canonicalize_tautomer_smiles(smi)\n",
        "\n",
        "        return_dict_valid[idx] = [smi2]\n",
        "\n",
        "\n",
        "def cal_fp(q, return_dict_fp):\n",
        "\n",
        "    nbits = 1024\n",
        "    while True:\n",
        "        qqq = q.get()\n",
        "        if qqq == 'DONE':\n",
        "            #            print('proc =', os.getpid())\n",
        "            break\n",
        "        idx, smi = qqq\n",
        "\n",
        "        if idx % 10000 == 0:\n",
        "            print(idx)\n",
        "        Nsmi = len(smi)\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            continue\n",
        "        if Chem.SanitizeMol(mol, catchErrors=True):\n",
        "            continue\n",
        "\n",
        "        com_fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=nbits)\n",
        "        return_dict_fp[idx] = [com_fp]\n",
        "\n",
        "\n",
        "def cal_sim(q, ref_data, return_dict_sim):\n",
        "\n",
        "    Nref = len(ref_data)\n",
        "    nbits = 1024\n",
        "    while True:\n",
        "        qqq = q.get()\n",
        "        if qqq == 'DONE':\n",
        "            #            print('proc =', os.getpid())\n",
        "            break\n",
        "        idx, smi = qqq\n",
        "\n",
        "        if idx % 10000 == 0:\n",
        "            print(idx)\n",
        "        Nsmi = len(smi)\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            continue\n",
        "        if Chem.SanitizeMol(mol, catchErrors=True):\n",
        "            continue\n",
        "\n",
        "        com_fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=nbits)\n",
        "        sim_data = []\n",
        "        for j in range(Nref):\n",
        "            ref_fp = ref_data[j][1]\n",
        "            sim = DataStructs.TanimotoSimilarity(com_fp, ref_fp)\n",
        "            sim_data += [sim]\n",
        "        similarity = np.array(sim_data)\n",
        "        j_max = similarity.argmax()\n",
        "        sim_max = similarity[j_max]\n",
        "        return_dict_sim[idx] = [sim_max, j_max]\n",
        "\n",
        "\n",
        "def cal_prop(q, return_dict_prop):\n",
        "\n",
        "    nbits = 1024\n",
        "    while True:\n",
        "        qqq = q.get()\n",
        "        if qqq == 'DONE':\n",
        "            #            print('proc =', os.getpid())\n",
        "            break\n",
        "        idx, smi = qqq\n",
        "\n",
        "#        if idx%10000==0:\n",
        "#            print(idx)\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        logP = MolLogP(mol)\n",
        "        SAS = sascorer.calculateScore(mol)\n",
        "        QED = qed(mol)\n",
        "        MW = MolWt(mol)\n",
        "        TPSA0 = TPSA(mol)\n",
        "\n",
        "        return_dict_prop[idx] = [logP, SAS, QED, MW, TPSA0]\n",
        "\n",
        "\n",
        "def main():\n",
        "    if len(sys.argv) < 1:\n",
        "        print(USAGE)\n",
        "        sys.exit()\n",
        "\n",
        "    data_dir = sys.argv[1]\n",
        "\n",
        "    Nproc = 30\n",
        "    gen_file = data_dir+\"/ARAE_smiles.txt\"\n",
        "    fp = open(gen_file)\n",
        "    lines = fp.readlines()\n",
        "    fp.close()\n",
        "    k = -1\n",
        "    gen_data = []\n",
        "    for line in lines:\n",
        "        if line.startswith(\"#\"):\n",
        "            continue\n",
        "        k += 1\n",
        "        smi = line.strip()\n",
        "        gen_data += [[k, smi]]\n",
        "\n",
        "    Ndata = len(gen_data)\n",
        "\n",
        "    q = Queue()\n",
        "    manager = Manager()\n",
        "    return_dict_valid = manager.dict()\n",
        "    proc_master = Process(target=creator, args=(q, gen_data, Nproc))\n",
        "    proc_master.start()\n",
        "\n",
        "    procs = []\n",
        "    for k in range(0, Nproc):\n",
        "        proc = Process(target=check_validity, args=(q, return_dict_valid))\n",
        "        procs.append(proc)\n",
        "        proc.start()\n",
        "\n",
        "    q.close()\n",
        "    q.join_thread()\n",
        "    proc_master.join()\n",
        "    for proc in procs:\n",
        "        proc.join()\n",
        "\n",
        "    keys = sorted(return_dict_valid.keys())\n",
        "    num_valid = keys\n",
        "\n",
        "    valid_smi_list = []\n",
        "    for idx in keys:\n",
        "        valid_smi = return_dict_valid[idx][0]\n",
        "        valid_smi_list += [valid_smi]\n",
        "\n",
        "    num_valid = len(valid_smi_list)\n",
        "\n",
        "    line_out = \"valid:  %6d %6d %6.4f\" % (\n",
        "        num_valid, Ndata, float(num_valid)/Ndata)\n",
        "    print(line_out)\n",
        "\n",
        "    unique_set = set(valid_smi_list)\n",
        "    num_set = len(unique_set)\n",
        "    unique_list = sorted(unique_set)\n",
        "\n",
        "    line_out = \"Unique:  %6d %6d %6.4f\" % (\n",
        "        num_set, num_valid, float(num_set)/float(num_valid))\n",
        "    print(line_out)\n",
        "\n",
        "    file_output2 = data_dir+\"/smiles_unique.txt\"\n",
        "    fp_out2 = open(file_output2, \"w\")\n",
        "    line_out = \"#smi\\n\"\n",
        "    fp_out2.write(line_out)\n",
        "\n",
        "    for smi in unique_list:\n",
        "        line_out = \"%s\\n\" % (smi)\n",
        "        fp_out2.write(line_out)\n",
        "    fp_out2.close()\n",
        "\n",
        "    ZINC_file = \"ZINC/train_5.txt\"\n",
        "    ZINC_data = [x.strip().split()[0]\n",
        "                 for x in open(ZINC_file) if not x.startswith(\"SMILES\")]\n",
        "    ZINC_set = set(ZINC_data)\n",
        "    novel_list = list(unique_set-ZINC_set)\n",
        "\n",
        "    novel_data = []\n",
        "    for idx, smi in enumerate(novel_list):\n",
        "        novel_data += [[idx, smi]]\n",
        "\n",
        "    q2 = Queue()\n",
        "    manager = Manager()\n",
        "    return_dict_prop = manager.dict()\n",
        "    proc_master = Process(target=creator, args=(q2, novel_data, Nproc))\n",
        "    proc_master.start()\n",
        "\n",
        "    procs = []\n",
        "    for k in range(0, Nproc):\n",
        "        proc = Process(target=cal_prop, args=(q2, return_dict_prop))\n",
        "        procs.append(proc)\n",
        "        proc.start()\n",
        "\n",
        "    q2.close()\n",
        "    q2.join_thread()\n",
        "    proc_master.join()\n",
        "    for proc in procs:\n",
        "        proc.join()\n",
        "\n",
        "    num_novel = len(novel_list)\n",
        "\n",
        "    line_out = \"Novel:  %6d %6d %6.4f\" % (\n",
        "        num_novel, num_set, float(num_novel)/float(num_set))\n",
        "    print(line_out)\n",
        "\n",
        "    file_output3 = data_dir+\"/smiles_novel.txt\"\n",
        "    fp_out3 = open(file_output3, \"w\")\n",
        "    line_out = '#SMILES logP SAS QED MW TPSA\\n'\n",
        "    fp_out3.write(line_out)\n",
        "    keys = sorted(return_dict_prop.keys())\n",
        "\n",
        "    for key in keys:\n",
        "        smi = novel_data[key][1]\n",
        "        prop = return_dict_prop[key]\n",
        "        logP, SAS, QED, MW, TPSA = prop\n",
        "        line_out = \"%s %6.3f %6.3f %5.3f %7.3f %7.3f\\n\" % (\n",
        "            smi, logP, SAS, QED, MW, TPSA)\n",
        "        fp_out3.write(line_out)\n",
        "    fp_out3.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "IOQSf3Gy_z1M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}