# MolGPT: Molecular Generation Using a Transformer-Decoder Model

### Abstract

SMILES notation(string of characters) → models in NLP(ex. Transformers)

**GPT**(Generative Pre-Training) models 영향

→ transformer-decoder on the next token prediction task

→ using masked SA for the generation of druglike molecules

**valid, novel, unique** molecules generation

trained **conditionally** to control multiple properties

generate molecules with **desired scaffolds** by conditioning the generation on **scaffold SMILES** strings of desired scaffolds and property values

(+) saliency maps → generative process of the model 해석

## Introduction

Problem

(1) Difficult to screen a practically infinite chemical space

(2) huge disparity between synthesized and potential molecules

**→ Deep generative models : modeling molecular distributions**

- **purpose** : sampling molecules that have desirable properties & activity
- learn probability distributions over a large set of molecules
- generate novel molecules by sampling from these distributions
- Molecular Sets(MOSES) & GuacaMol datasets

Models like **RNN** - trained on a large corpus of molecules + focused through RL or transfer learning

**Auto-Encoder variants(VAE,AAE)** - generated by sampling from latent spaces 
/ Randomization of SMILES가 data augmentation strategy로 사용되기도 함.

**Junction Tree VAE(JT-VAE)** - represent molecules as graph tree structures
/ ensure 100% **validity** of generated molecules by maintaining a vocabulary of molecular components that can be added at each junction. 모든 generation step마다 validity 확인.

**GANs**

- can generate highly realistic content
- ORGAN : first usage
- RANC : introduce RL alongside a GAN loss to generate molecules of desirable properties
- **LatentGAN** : use latent vectors as input and outputs
    - mapped to molecules by the decoder of a pre-trained autoencoder
    - ensure that model can work with latent representations, not handling SMILES syntax

**Others** (Bayesian optimization, RL)

- MolCycleGAN : utilize the **JT-VAE** architecture, applies CycleGAN loss + given molecule templates
- Conditional RNN, DING, CARAE : sample molecules based on exact values(user defined property values)
- graph based method : ensure the presence of desired scaffolds + exact property values

**Transformer**

- language translation tasks
- encoder module : gains context from all input tokens through SA mechanism
- decoder module : predict next token with context (from both the encoder, previously generated tokens by attention)

**GPT**(Generative Pre-Training Transformer)

- develop better language embeddings that model longer-distance connections
- multiple language modeling tasks(natural language inference, QA, sentence similarity, classification)

### MolGPT

- predict a sequence of SMILES tokens for molecular generation
- regular expression(SMILES tokenizer) - breaks SMILES strings into a set of relevant tokens
- attention applied to all previously generated tokens → SMILES grammar
- controls user-specified molecular properties and scaffolds with good accuracy

## Methods

### Datasets

- MOSES
    - 1.9 million clean lead-like molecules from ZINC data seta (clean Lead Collection)
    - molecular weight ranging from 250~350 Da, number of rotatable bonds lower than 7, XlogP below 3.5
    - created mainly to represent lead like molecules → distribution of molecules + ideal druglike properties
- GuacaMol
    - subset of ChEMBL 24 database contains 1.6 million molecules
    - used to test property conditional generation - larger distribution of property values available in the dataset

**RDkit** → calculate molecular properties, extract BM(Bemis-Murcko) scaffolds

**Properties** of molecules used to control generation and optimization

: logP, SAS, TPSA, QED(Quantitative Estimate of Drug-likeness)

### Model Overview

- non-conditioned training
    
    tokenized using a SMILES tokenizer → predict next token
    
- property conditioned & scaffold conditioned training
    
    RDkit 이용해 properties, scaffolds 추출해 condition으로 전달
    
    예측된 tokens : 이전 분자 token뿐만 아니라 condition에 attention한 결과
    
    - property conditions
        
        분리된 FC linear layer(condition을 256-차원 vector에 mapping) → 높은 차원에서 properties 표현
        
        resultant vector가 SMILES embedding sequence의 처음부터 연결되기 시작
        
    - scaffold conditions
        
        같은 embedding layer을 사용하여 scaffold string의 각 token을 256-차원 vector에 mapping
        
        resultant vector가 SMILES embedding sequence의 처음부터 연결되기 시작
        
- **generation 과정**
    - start token을 모델에 제공
        - weighted random samplling ← the list of first tokens of molecules in SMILES training set
        - **weight** : SMILES training set의 첫번째 위치에서 발생하는 빈도에 의해 결정
        - **SMILES tokenizer** 사용, 반복적 사용 위해 **utils.py**의 (SMILES iterator & SMILES enumerator & randomize_smiles & canonic)
    - start token부터 시작해 순차적으로 다음 token 예측하여 분자 생성
    - property & scaffold condition도 같이 제공하여 분자 sampling

mini version of GPT (6M parameters)

- **8 stacked decoder blocks - masked SA layer + FC NN**
- SA layer → 256-sized vector → hidden layer → 1024-sized vector → GELU activation layer → FC NN → 256-sized vector
- **Embedding** : position value embedding + segment token embedding(for conditional training)
    
    position value embedding for input sequnece 순서 유지
    
    segment token embedding : condition과 SMILES token 구별하기 위해 제공
    
    - represent - 특정 input이 condition인지, molecule SMILES token인지
    
    모든 SMILES token은 256-차원 vector에 embedding layer을 사용하여 mapping됨.
    
    분리된 trainable embedding layers → map position tokens & segment tokens → 256-차원 vector → 2가지 SMILES token embedding 추가 → SMILES string 각 token마다 256-sized vector → input
    
- SA는 Scaled Dot Product Attention 통해 계산됨. 각 token마다 q,k,v vector 존재
    - query vector : query the weights of each individual value vector. key vectors에 의해 dot product로 처음 전송
        
        dot product : scaled by dimensions of vectors
        
        softmax function이 대응되는 weights를 얻기 위해 적용
        
    - value vector : 각각 weights에 대해 곱해지고 더해짐.
    - 각 decoder block의 가중치 행렬에 따라 3 vector들이 계산됨.
    - **SA : sequence의 모든 token들에 attention 제공 → sequence의 다음 token 예측하려고 모델을 훈련시킬 때는 좋지 않음.**
- Masked SA : mask attention to all sequence tokens that occur in future time steps
- **multiple masked SA operations** → better representations by attending to different representation subspaces at different positions

### Training Procedure and Evaluation Metrics

- training procedure
    - 10 epochs + Adam optimizer + learning rate 6*10^(-4)
    - start token 제공
    - check MolGPT’s capacity - control molecular properties, core structures
    - NVDIA 2080Ti GPU
- Evaluation metrics
    - **Validity** : (number of valid molecules) / (number of generated samples)
        
        use RDkit for validity check / how well the model has learned the SMILES grammar and the valency of atoms (satisfy valencies & ring closures)
        
    - **Uniqueness** : (number of unrepeated molecules) / (number of valid molecules)
        
        낮을수록 repetitive generation, low level of distribution learning
        
    - **Novelty** : (number of molecules - not included in the training set) / (number of unique molecules)
    - **Internal Diversity(IntDiv_p) :** 생성된 분자들의 diversity(mode collapse나 similarity 측정), Tanimoto similarity 이용.
    - **Frechet ChemNet Distance(FCD)** : ChemNet model의 penultimate layer에서 얻은 molecular features를 사용하여 측정
    - **KL divergence**
        - calculated using numerous physicochemical descriptors of the generated & reference sets
        - 낮을수록 모델이 properties의 distribution 잘 학습하고 있음.
        - properties k → final score S 측정

## Results and Discussion

**Nonconditioned Molecular generation**

- Internal diversity scores → extent of chemical space traversed by different models
- FCD & KL divergence → how well the model captures the statistics & distribution of features (각각)
- model에서 생성된 valid molecules에 대해서만 계산. high validity & long-range dependencies
- MOSES에 대해 MolGPT가 가장 좋은 FCD score. explicit constraints 없이 거의 perfect validity scores ↔ low novelty score
- Guacamol에 대해 validity, novelty, KL divergence 좋은 성능 - MOSES보다 large molecules

**Generation based on Single and Multiple properties**

- exhibit specific properties(conditional generation) → accuracy 필요
- property control : logP, SAS, TPSA, QED (2D 구조에서 추론 가능한 특성이면 상관없음.)
- ability to control multiple properties 동시에 → logP, SAS, TPSA 사용 → 정확한 생성 가능

**Generation based on scaffolds**

- sepcific scaffold/skeleton 포함한 분자 생성 (structures with certain property values)
- random set of 100 test scaffolds, 각각 100개 분자 생성 → validity, uniqueness, novelty, similarity ratio
    - similarity ratio - fraction of valid generated molecules + scaffold의 Tanimoto similarity + 0.8 넘는 conditioned scaffold
    - Murcko scaffolds의 fingerprints로부터 Tanimoto similarity 계산

**Generation based on scaffold and property**

- generate structures containing desired scaffolds + control multiple molecular properties
- 5 scaffolds of different sizes - randomly chosen from MOSES test set
- SSF(Same Scaffold Fraction) : condition에서 같은 scaffold를 가진 생성 분자들의 비율
- QED : dependent on multiple molecular properties simultaneously → scaffold 구조에 영향을 받아 constraints에서 control하기 어려움.

## Conclusion

MolGPT : Transformer-Decoder model

- **masked SA** mechanisms → simple to learn **long-range dependencies** between string tokens
- useful to learn semantics of **valid** SMILES (satisfy valencies & ring closures)
- good performance → GuacaMol data set in terms of validity and novelty
    - high validity & uniqueness scores
    - good FCD & KL divergence - both datasets
- higher level chemical representations + molecular property control
    - exact values passed by the user
    - user-specified scaffolds controlling properties - good accuracy